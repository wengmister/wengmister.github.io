<div class="modal-section overview-section">
    <div class="overview-text">
        <h3 class="modal-section-title">Overview</h3>      
        <p>
            Building on the 
            <a href="https://wengmister.github.io/#dexterous-hand" target="_blank">BiDexHand</a> 
            project, I programmed the robotic hand to play classical music by interpreting MIDI scores.
        </p>
        
        <p>Enabling a robot to play the piano introduces several new technical challenges:</p>
        <ul>
            <li>Closed-loop, calibrated position control with +/-5mm fingertip accuracy</li>
            <li>High-bandwidth, low-latency motion control</li>
            <li>Efficient task scheduling and execution</li>
        </ul>
        
        <p>
            In the following sections, I'll discuss how I augmented the BiDexHand to meet these 
            requirements, and walk through the control architecture I designed for the robot to 
            perform music.
        </p>
    </div>
    <div class="overview-image">
        <iframe 
            width="400" 
            height="400" 
            src="https://www.youtube.com/embed/3TNHTjh7_L4?si=5oqtpg4i6PREGAlJ&amp;controls=0" 
            title="YouTube video player" 
            frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            referrerpolicy="strict-origin-when-cross-origin" 
            allowfullscreen>
        </iframe>
    </div>
</div>

<div class="modal-section">
    <h3 class="modal-section-title">But first... Why?</h3>
    <div class="modal-section overview-section">
        <div class="overview-text">
            <p>
                As a lifelong piano player, I've always wanted to build a robot that could share the keyboard with me. 
                Piano playing to me is <em>the</em> epitome of hand dexterity - it requires both precise, and timely finger movements 
                to make the music notes flow.
            </p>
            <p>
                Nevertheless, making a robot play the piano with human-like dexterity is an extremely challenging task that not only 
                presents hardware challenges, but also requires a sophisticated control architecture. 
            </p>
            <p>
                This project is a step towards that goal. It demonstrates how a robot can be programmed to interpret 
                structured, baroque-style music and play it with a high degree of accuracy.
            </p>
        </div>
        <div class="overview-image">
            <img src="assets/images/project_images/bdbach/pianos.gif" alt="B.D. Bach Overview" loading="lazy">
        </div>
    </div>

<div class="modal-section">
    <h3 class="modal-section-title">Hardware Update</h3>
    <div class="modal-section overview-section">
        <div class="overview-image">
            <img 
                src="assets/images/project_images/bdbach/phalanx++ 2.gif" 
                alt="phalanx update" 
                loading="lazy">
        </div>
        <div class="overview-text">
            <h2 class="modal-section-sub-title">Phalanx Redesign</h2>
            <p>
                To improve accuracy and structural robustness, I redesigned the BiDexHand's phalanges.
            </p>
            <p>
                The new design uses a single-shear 4-bar linkage for the PIP-DIP coupling. 
                This approach avoids the need for coaxial joints, which eliminates the risk 
                of tolerance-induced joint binding and creates more room for placing AprilTags.
            </p>
            <p>
                Additionally, I redesigned the components to use precision shoulder screws
                and sleeve bearings, resulting in better and more reliable shaft fits.
            </p>
        </div>
    </div>
    
    <div class="modal-section overview-section">
        <div class="overview-text">
            <h2 class="modal-section-sub-title">Mechatronics Overhaul</h2>
            <p>
                Achieving the required accuracy and bandwidth necessitated 
                an upgrade to the BiDexHand's servo control system.
            </p>
            <p>
                The servo control system now uses FeeTech's SCS series servos. 
                Their half-duplex asynchronous serial bus allows me to index 
                and control the servos with higher precision, greater torque, 
                and a wider range of motion. I also created simple PCB breakout boards 
                to daisy-chain the servos, which helps with cable management inside the hand.
            </p>
            <p>
                As an added bonus, the SCS servos expose a range of sensor feedback, 
                allowing me to monitor load, position, and temperature on each joint.
            </p>
        </div>
        <div class="overview-image">
            <img 
                src="assets/images/project_images/bdbach/scs_bus.png" 
                alt="Servo Control" 
                loading="lazy">
        </div>
    </div>
</div>

<div class="modal-section">
    <h3 class="modal-section-title">Joint Calibration</h3>
    <div class="modal-section overview-section">
        <div class="overview-image">
            <img 
                src="assets/images/project_images/bdbach/calibration.gif" 
                alt="Calibration" 
                loading="lazy">
        </div>
        <div class="overview-text">
            <p>
                To achieve the necessary accuracy, I designed a calibration algorithm 
                that approximates the cable-driven joints with linear models.
            </p>
            <p>
                Using AprilTag fiducials, I calibrated the robotic hand by mapping 
                servo commands to relative joint angles. Finding transformations between 
                successive tag readings generates a sufficiently accurate model to describe the joint motion.
            </p>
            <p>
                By leveraging the fact that the tags are normal to their attached planes, 
                I can also determine the ideal offset to their nominal positions.
            </p>
        </div>
    </div>
</div>

<div class="modal-section">
    <h3 class="modal-section-title">Inverse Kinematics</h3>
    <div class="modal-section overview-section">
        <div class="overview-text">
            <p>
                The robot hand needs to be able to hit the desired key with the best finger tip accurately in order to play a song.
                Since BiDexHand has underactuated fingers with mechanical linkages, I developed an IK module that numerically solves the IK problem.
            </p>
            <p>
                Using the parsed robot URDF as a reference, the kinematics module maintains a tree of 
                available kinematic chains and provides kinematics services to other modules. 
                When called, the module can solve for the ideal finger and thumb joint angles to reach a target position within 10ms.
            </p>
        </div>
        <div class="overview-image">
            <img src="assets/images/project_images/bdbach/IK.gif" alt="Inverse Kinematics" loading="lazy">
        </div>
    </div>
</div>

<div class="modal-section">
    <h3 class="modal-section-title">FER Robot Arm Control</h3>
    <div class="modal-section overview-section">
        <div class="overview-image">
            <img src="assets/images/project_images/bdbach/arm_control.gif" alt="Robot Arm Control" loading="lazy">
        </div>
        <div class="overview-text">
            <p>
                I used MoveIt!2 to command the Franka FER Robot arm, allowing for kinematic control of the end-effector flange's position and orientation.
            </p>
            <p>
                This layer of control delivers the hand to a position close to the keyboard, allowing the hand to then perform its own 
                IK calculations to move the fingers and play the notes.
            </p>
            <p>
                While playing music, the robot arm dynamically adjusts the end-effector's position based on the average position 
                of notes within a timed window, ensuring that the desired key is always within the hand's reach.
            </p>
        </div>
    </div>
</div>

<div class="modal-section">
    <h3 class="modal-section-title">Continued Improvements</h3>
    <div class="modal-section overview-section">
        <div class="overview-text">
            <p>This is still a project under development. In the near future, I plan to experiment with and implement:</p>
            <ul>
                <li>Learning-based whole arm control</li>
                <li>More complex wrist movements to support transitions</li>
                <li>More expressive playing with dynamics and articulation</li>
            </ul>
        </div>
        <div class="overview-image">
            <img src="assets/images/project_images/bdbach.gif" alt="B.D. Bach Playing" loading="lazy">
        </div>
    </div>
</div>