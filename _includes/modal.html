<!-- Modal window -->
<div id="project-modal" class="modal">
    <div class="modal-overlay"></div>
    <div class="modal-content">
        <button class="modal-close">&times;</button>
        <div class="modal-body">
            <h2 class="modal-title"></h2>
            <div class="modal-external-links"></div>
            <div class="modal-description"></div>
        </div>
    </div>
</div>

<script>
    const projectDetails = {
        'monarch': {
            title: 'Monarch Endoscopic Surgical Platform',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                                <p>The Monarch Platform is a major breakthrough in robotic-assisted surgery, especially for minimally invasive procedures involving the lungs and kidneys.</p>
                                <p>As a senior mechanical engineer currently working at J&J, I am mainly responsible for system development of the Monarch Surgical Platform, the first robotic platform for minimally invasive diagnostic and therapeutic procedures in the lungs and kidneys.</p>
                                <p>I work on a variety of projects focused on the robotic arm and instrument manipulator, addressing both hardware and software challenges. Additionally, I support the hardware team in advancing the overall platform development.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Auris1.png" alt="Monarch Platform" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Robotic Arm Design',
                    'Medical Device',
                    'Robot Arm Calibration',
                    'NPS Software Development'
                ],
                externalLinks: [
                    { text: 'Company Website', url: 'https://www.jnjmedtech.com/en-US/product-family/monarch/' }
                ]
            }
        },
        'yomi': {
            title: 'Yomi Dental Surgical Robot',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                                <p>At Neocis, I was heavily involved in developing the second generation of the Yomi surgical system, the only FDA-cleared robotic device for dental implant surgery. My major contributions included:</p>
                                <ul>
                                    <li>Design and development of joint actuators for a 7-DOF robotic arm</li>
                                    <li>Creation of compact robot end-effector for tool integration and user interaction</li>
                                    <li>Implementation of kinematic analysis and simulation for joint load optimization</li>
                                    <li>Development of calibration algorithms and inverse kinematics solver to enable obstacle avoidance capabilities</li>
                                </ul>
                                <p>The Yomi system provides haptic guidance to dental surgeons while maintaining their direct control of the surgical instruments, combining the precision of robotic technology with the expertise of human practitioners.</p>  
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Neocis1.png" alt="Yomi Dental Robot" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Robotic Actuator Design',
                    'Precision Mechatronics',
                    'Robot Calibration',
                    'System Integration'
                ],
                externalLinks: [
                    { text: 'Company Website', url: 'https://www.neocis.com/' }
                ]
            }
        },
        'harmony': {
            title: 'Harmony SHR Exoskeleton',
            sections:{
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                                <p>During my time at Harmonic Bionics, I contributed to the development of the Harmony SHR, an advanced upper extremity rehabilitation robot. My responsibilities included:</p>
                                <ul>
                                    <li>Design of major mechatronic systems using various manufacturing techniques</li>
                                    <li>Development of handheld interface devices and custom linear actuator systems</li>
                                    <li>Material validation through simulation and experimental testing</li>
                                    <li>Integration of mechanical and electrical systems for prototype development</li>
                                </ul>
                                <p>The Harmony SHR is designed to facilitate natural movement patterns during rehabilitation, allowing therapists to deliver high quality care while collecting objective patient data.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/HB1.png" alt="Harmony SHR" loading="lazy">
                        </div>
                    </div>
                    `,
                technology: [
                    'Exoskeleton Design',
                    'Rehabilitation Robotics',
                    'Linear Actuators',
                    'Human-Robot Interface'
                ],
                externalLinks: [
                    { text: 'Company Website', url: 'https://harmonicbionics.com/' }
                ]
                // keyFeatures: [
                //     'Upper Extremity Rehabilitation',
                //     'Custom Linear Actuators',
                //     'Patient Data Collection',
                //     'Natural Movement Patterns'
                // ]
            }
        },
        'athena': {
            title: 'Athena Biomimetic Robot',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>As team lead for the Athena project at Georgia Tech's LIDAR Lab, I managed the development of a complex biomimetic upper body robot:</p>
                            <ul>
                                <li>Led a team of 7 undergraduate students in system design and integration</li>
                                <li>Implemented a 28-DOF system using linear actuators to mimic human muscle architecture</li>
                                <li>Managed CAD repositories and performed topology optimization for weight reduction</li>
                            </ul>
                            <p>The Athena project aims to advance our understanding of human-like movement and control in robotic systems, with potential applications in prosthetics and human-robot interaction.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/LIDAR1.png" alt="Athena Biomimetic Robot" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Biomimetic Design',
                    'Linear Actuation',
                    'CAD Optimization',
                    'System Integration'
                ],
                externalLinks: [
                    { text: 'Lab Homepage', url: 'https://lab-idar.gatech.edu/' }
                ]
                // keyFeatures: [
                //     '28-DOF System',
                //     'Muscle-like Actuation',
                //     'Topology Optimization',
                //     'Team Leadership'
                // ]
            }
        },
        'hip-exo': {
            title: 'Assistive Hip Exoskeleton',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>Working in the EPIC Lab at Georgia Tech, I contributed to the development of an assistive hip exoskeleton designed to reduce the metabolic cost of walking:</p>
                            <ul>
                                <li>Design and fabrication of series elastic actuator housings and components</li>
                                <li>Development of experimental protocols for system characterization</li>
                                <li>Collection and analysis of metabolic cost data under various conditions</li>
                            </ul>
                            <p>This research aimed to develop more effective assistive devices for individuals with mobility impairments, with a focus on improving energy efficiency during walking.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/EPIC1.png" alt="Assistive Hip Exoskeleton" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Exoskeleton Design',
                    'Series Elastic Actuators',
                    'Biomechanics',
                    'Data Collection'
                ],
                externalLinks: [
                    { text: 'Lab Homepage', url: 'https://www.epic.gatech.edu/' }
                ]
                // keyFeatures: [
                //     'Metabolic Cost Reduction',
                //     'Custom Actuator Design',
                //     'Performance Analysis',
                //     'User Testing'
                // ]
            }
        },
        'dexterous-hand': {
            title: 'Biomimetic Dexterous Hand',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>The human hand, with its 27 bones, intricate network of tendons and muscles, and over 20 degrees of freedom, is a marvel of dexterity and adaptability. It seamlessly transitions between delicate tasks, like threading a needle, and powerful actions, such as gripping heavy objects. Inspired by this remarkable complexity, I aim to design a robotic hand that not only replicates its functionality but also captures its versatility and precision.</p>
                            <p>Started in January 2025, the Biomimetic Dexterous Hand project is a work-in-progress. I will continue to post updates to the project here as it develops.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/hand_intro.png" alt="Biomimetic Dexterous Hand" loading="lazy">
                        </div>
                    </div>
                `,
                'Phalanx Design': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/iterations.png" alt="Phalanx iterations" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>Developing the Hand involves a heavily iterative design process. </p>
                            <p>I designed a 4-bar linkage system to replicate the coupled movement of the distal and middle interphalangeal joints. This mechanism faithfully mimics the natural rhythm of the human finger while reducing the actuator count by one per phalanx, thereby streamlining the design without sacrificing functionality.</p>
                            <p>After five iterations, I arrived at the current phalanx design that balances robust strength with an extensive range of motion. Each prototype iteration provided critical feedback, allowing for adjustments that enhance the hand's reliability and dynamic performance in real-world applications.</p>
                        </div>
                    </div>
                `,
                'Cable-driven Mechanism': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>The cable-driven mechanism is the heart of the hand design. It provides precise control over joint movement while minimizing distal inertia - a crucial factor for achieving human-like dexterity.</p>
                            <p>By using a tendon-driven system, I can actuate the phalanx with minimal force, achieving a more natural movement pattern. The mechanism is designed to be compact and lightweight, ideal for a dexterous hand.</p>
                            <p>To achieve the desired movement, I'm using miniaturized motors and PTFE tubing-based cable routing. This combination minimizes friction, enhances durability, and maintains a design that is both modular and efficient for executing complex tasks.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/phalanx.gif" alt="Biomimetic Dexterous Hand" loading="lazy">
                        </div>
                    </div>
                `,
                'Motion Shadowing': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/motion_shadowing.gif" alt="Motion Shadowing" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>One of the key features of the Hand is its ability to shadow human hand movements. By using computer vision and machine learning algorithms, the Hand can mimic human gestures with high precision.</p>
                            <p>To implement this feature, I wrote a ROS2 node that uses Google's MediaPipe framework to acquire hand landmark position and calculate joint angles via vector math. These computed angles are then published to the Hand's servo controller through a hand angle topic, ensuring seamless synchronization between human intent and robotic motion.</p>
                            <p>Stay tuned for more updates on this exciting journey!</p>
                        </div>
                    </div>
                `,
                'Control Architecture':`
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>The control architecture for the Hand is designed and implemented with ROS2, allowing for easy integration with the pub-sub systems.</p>
                            <p>A few ROS2 nodes work together to implement the control: vision node integrates MediaPipe and processes camera input; application node processes joint angles and compensates them according to robot calibration; and finally the servo node sends the corresponding commands to the Hand's servo controller via serial comms.</p>
                            <p>By decoupling the control logic from the hardware, I ensure that the Hand can be easily adapted to different applications and environments. This flexibility is essential for future development and integration with other robotic systems.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/control_architecture.png" alt="Control Architecture" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Mechatronics Design',
                    'Cable Driven Actuators',
                    'Rapid Prototyping',
                    'Computer Vision'
                ],
                externalLinks: [
                    { text: 'Project Repo', url: 'https://github.com/wengmister/Dex_Hand_MSR' }
                ]
            }
        },
        'robotic-mini-golf': {
            title: 'Apex Putter - Robotic Mini-golf',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>Mini golf offers a fascinating challenge for robotics. While children rely on intuition and practice to master it, programming a robot to achieve the same goal demands precise control over its movements and timing. A child needs only a putter, a ball, and a hole to score a hole-in-one, but for a robot, careful planning and execution are key.</p>
                            <p>For the Franka Panda robot, this task becomes even more intricate. Although the robot doesn’t feel the pressure of competition, a poorly planned Cartesian path can create significant difficulties. In this setup, the robot must putt the ball with just the right amount of force to successfully reach the hole.</p>
                            <p>In this project, I developed the vision module for the robot, designed the mechanical components for the putter end-effector, wrote the demo tasks, and created the ROS2 package for Apex-Putter.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP.png" alt="Robotic mini-golf" loading="lazy">
                        </div>
                    </div>
                `,
                Vision: `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP_vision2.png" alt="Apriltag system" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>The vision module is one of the two major components of this robot. Using a single RealSense D435 camera, this module fulfills two key functions: registering the robot base in the camera frame and tracking the ball and hole within the same frame.</p>
                            <p>To achieve this, I used OpenCV to process the camera image and AprilTags to locate the robot base. I designed an AprilTag mount to provide an initial estimate of the robot base transformation and performed optical calibration using the Kabsch algorithm to improve accuracy.</p>
                        </div>
                    </div>
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>After determining the initial robot base transformation, I used a YOLOv8 model to detect the ball and hole in the camera frame.</p>
                            <p>To enhance the model’s prediction accuracy, I trained it on a custom dataset created with images captured by the same camera. The trained model then identified the ball and hole positions in the camera frame.</p>
                            <p>With the camera frame positions of all necessary elements established, I registered them using tf2, enabling me to retrieve any required transformation with a single command.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP_vision3.png" alt="YOLO" loading="lazy">
                        </div>
                    </div>
                `,
                'Mechanical Integration': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP_mech1.png" alt="Mechanical Integration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>To integrate effectively with the Franka FER arm, I designed a custom end-effector specifically for the robot.</p>
                            <p>This end-effector seamlessly interfaces with off-the-shelf putter components. The design underwent several iterations to improve alignment, simplify kinematic planning, and optimize the putter face for smoother motion.</p>
                        </div>
                    </div>
                `,
                'Motion Control': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>The motion control module leverages ROS2 to interface with the robot and the vision module.</p>
                            <p>Using data from the vision module, the motion control module calculates the robot’s end-effector position and orientation, then plans a precise vector path to move the putter toward the ball and hole.</p>
                            <p>As the path is planned, the node uses MoveIt2! package to send the kinematic planning to the robot, which executes the motion accordingly.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/apex-putter.gif" alt="Motion Control" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Kinematic Control',
                    'Computer Vision',
                    'Machine Learning',
                    'YOLO'
                ],
                externalLinks: [
                    { text: 'Project Repo', url: 'https://github.com/wengmister/Apex-Putter'}
                ]
            }
        },
        'grab-a-pen': {
            title: 'Robot Arm Grab-a-pen - MSR Hackathon',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>For the final challenge of the MSR Hackathon, we were tasked with using a RealSense 435D RGBD camera and a Trossen PincherX100 robot arm to locate and grab a pen within the arm's workspace.</p>
                            <p>Though the task seemed straightforward, it required a thorough understanding of the robot's kinematics, the camera's calibration, and the integration of both systems.</p>
                            <p>For this project, I developed a custom calibration system for both the robot arm and the camera, and wrote the motion control sequence in Python. As a bonus, I collaborated with my cohort mate <a href="https://absrat.com/" target="_blank">Zhengxiao Han</a> to pass the pen between our robotic arms.</p>                           
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/grab-a-pen/setup.png" alt="MSR Challenge" loading="lazy">
                        </div>
                    </div>
                `,
                'Vision and Calibration': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/grab-a-pen/calibration.gif" alt="Optical calibration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>To determine the position of the pen in the camera's frame, I used the OpenCV library to process the camera image and locate the pen using HSV color thresholding. This enabled me to deproject the centroid of the pen (highlighted in purple) into 3D space.</p>
                            <p>To register the camera frame with the robot base frame, I used the pen itself as a marker. By moving the pen within the robot's end-effector, I performed an optical calibration. After collecting corresponding points from both the robot and the camera, I applied the Kabsch algorithm to calculate the optimal transformation between the two frames.</p>
                        </div>
                    </div>
                `,
                'Motion Control': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>Trossen Robotics provides a Python API for the PincherX100 robot arm, which I used to send joint commands directly to the robot.</p>
                            <p>Using the pen's position in the camera frame, I calculated the inverse kinematics needed for the robot arm to reach the pen. I then sent joint commands to the robot to position the end-effector at the pen's location.</p>
                            <p>Once the robot arm reached the pen, I programmed it to grasp the pen and pass it to another robot arm. This required precise control over both the end-effector and joint positions.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/grab-a-pen/execute.gif" alt="Motion Control" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Computer Vision',
                    'Kinematic calibration',
                    'Hand-eye registration',
                    'Kinematic control'
                ],
                externalLinks: [
                    { text: 'Project Repo', url: 'https://github.com/wengmister/msr_PincherX'}
                ]
            }
        },
        'mobile-manipulation': {
            title: 'Mobile Manipulation - ME449 Robotic Manipulation',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>Mobile Manipulation was the final challenge of the ME449: Robotic Manipulation course at Northwestern University.</p>
                            <p>Using robotic manipulation, forward kinematics, and linear control techniques we learned in class, I created a simulation in CoppeliaSim. The simulation demonstrated a KUKA youBot moving a box from one location to another with precision and efficiency.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/mobile-manipulation/youbot.jpg" alt="Kuka youBot" loading="lazy">
                        </div>
                    </div>
                `,
                'Path Planning': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/mobile-manipulation/gripper.png" alt="Youbot gripper configuration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>Given the desired initial and final target positions and orientations, I first generated the target trajectory using quintic polynomial interpolation.</p>
                            <p>Next, I constructed the body Jacobian for the KUKA youBot and used its pseudoinverse to calculate the necessary configurations to move the robot from its current state to the target position.</p>
                        </div>
                    </div>
                `,
                'Feedforward Control': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>Using the calculated joint configurations, I designed a PI controller to move the robot from its initial to its desired configuration. The controller was tuned to minimize 6-dimensional motion control errors and ensure smooth motion.</p>
                            <p>The simulation works by computing the difference between the desired and actual joint configurations. The PI controller then generates a control signal to move the robot to the desired configuration through Euler integration.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/mobile-manipulation/errors.png" alt="Control errors" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Robotic Manipulation',
                    'Forward Kinematics',
                    'Linear Control',
                    'Simulation'
                ],
                externalLinks: [
                    { text: 'Project Repo', url: 'https://github.com/wengmister/me449/tree/main/final_project'}
                ]
            }
        },
        'point-cloud-classification': {
            title: 'RGB Point Cloud Classification - Machine Learning',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>RGB Point Cloud Classification was my final project for MSAI 349: Machine Learning.</p>
                            <p>Point clouds are a common data format used in robotics and computer vision. In this project, we implemented a custom PointNet architecture and augmented it with RGB color on the point cloud to classify objects in a custom dataset we generated.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/point-cloud/pcc-overview.gif" alt="Point Cloud Classification" loading="lazy">
                        </div>
                    </div>
                `,
                'PointNet Architecture': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/point-cloud/pcc-intro.gif" alt="PointNet Introduction" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>PointNet is a deep learning architecture designed to process point clouds directly. It is invariant to input permutation and can learn local features from unordered point sets.</p>
                            <p>For our project, we implemented a custom PointNet architecture using PyTorch. The network was trained on a custom dataset of point clouds augmented with RGB color information.</p>
                        </div>
                    </div>
                `,
                'RGB Augmentation': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>RGB color information provides valuable context for object classification in point clouds. By augmenting our dataset with RGB values, we enhanced the network's ability to distinguish between objects of similar shape.</p>
                            <p>During training, we concatenated the RGB values with the point cloud data and fed the combined input to the PointNet architecture. This allowed the network to learn color-based features in addition to geometric features.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/point-cloud/pcc-architecture.png" alt="RGB Augmentation" loading="lazy">
                        </div>
                    </div>
                `,
                'Acknowledgement': `
                    <div class="modal-section overview-section">
                            <p>Special thanks to my project partners, <a href="https://absrat.com/" target="_blank">Zhengxiao Han</a>, <a href="https://benbenyamin.github.io/" target="_blank">Ben Benyamin</a> and mentor <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/demeter-david.html" target="_blank">Prof. David Demeter</a> for their collaboration and contributions to this project.</p>
                    </div>
                `,
                technology: [
                    'Machine Learning',
                    'Multilayer Perceptron',
                    'PointNet',
                    'RGB Augmentation'
                ],
                externalLinks: [
                    { text: 'Project Repo', url: 'https://github.com/msr-in-msai/msai-349-final-project'}
                ]
            }
        }
    };
    
    document.addEventListener('DOMContentLoaded', () => {
        const modal = document.getElementById('project-modal');
        const modalTitle = modal.querySelector('.modal-title');
        const modalExternalLinks = modal.querySelector('.modal-external-links');
        const modalDescription = modal.querySelector('.modal-description');
        const closeButton = modal.querySelector('.modal-close');
        const overlay = modal.querySelector('.modal-overlay');

        function openModal(projectId) {
        const project = projectDetails[projectId];
        if (project) {
            // Set modal title
            modalTitle.textContent = project.title;

            // Render external links (if available) right below the title
            if (project.sections.externalLinks) {
            modalExternalLinks.innerHTML = project.sections.externalLinks
                .map(link => `<a href="${link.url}" target="_blank" class="contact-item">${link.text}</a>`)
                .join(' ');
            } else {
            modalExternalLinks.innerHTML = '';
            }

            // Generate the remaining modal content dynamically
            let modalContent = '';
            for (const [sectionKey, sectionValue] of Object.entries(project.sections)) {
            // Skip externalLinks since it's handled separately
            if (sectionKey === 'externalLinks') continue;

            if (sectionKey === 'overview') {
                modalContent += sectionValue; // Use preformatted overview
            } else if (sectionKey === 'technology') {
                modalContent += `
                <div class="modal-section">
                    <h3 class="modal-section-title">Technology Stack</h3>
                    <div class="tech-list">
                    ${sectionValue.map(tech => `<span class="tech-item">${tech}</span>`).join('')}
                    </div>
                </div>
                `;
            } else {
                modalContent += `
                <div class="modal-section">
                    <h3 class="modal-section-title">${capitalize(sectionKey)}</h3>
                    <div>${sectionValue}</div>
                </div>
                `;
            }
            }
            modalDescription.innerHTML = modalContent;

            modal.classList.add('active');
            document.body.classList.add('modal-open');

            // Update the URL hash
            history.pushState({ projectId: projectId }, '', `#${projectId}`);
        }
        }

        function closeModal() {
        modal.classList.remove('active');
        document.body.classList.remove('modal-open');
        // Revert the URL
        history.pushState(null, '', window.location.pathname);
        }

        function capitalize(str) {
        return str.charAt(0).toUpperCase() + str.slice(1);
        }

        // Add click listeners to project items
        document.querySelectorAll('.project-item').forEach(item => {
        item.addEventListener('click', () => {
            const projectId = item.getAttribute('data-project-id');
            openModal(projectId);
        });
        });

        // Close modal when clicking the close button or overlay
        closeButton.addEventListener('click', closeModal);
        overlay.addEventListener('click', closeModal);

        // Close modal on escape key press
        document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape') {
            closeModal();
        }
        });

        // Handle back button
        window.addEventListener('popstate', (event) => {
        if (event.state && event.state.projectId) {
            openModal(event.state.projectId);
        } else {
            closeModal();
        }
        });

        // Check URL hash on page load
        const initialProjectId = window.location.hash.substring(1);
        if (initialProjectId && projectDetails[initialProjectId]) {
        openModal(initialProjectId);
        }
    });
</script>
