<!-- Modal window -->
<div id="project-modal" class="modal">
    <div class="modal-overlay"></div>
    <div class="modal-content">
        <button class="modal-close">&times;</button>
        <div class="modal-body">
            <h2 class="modal-title"></h2>
            <div class="modal-description"></div>
        </div>
    </div>
</div>

<script>
    const projectDetails = {
        'monarch': {
            title: 'Monarch Endoscopic Surgical Platform',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>The Monarch Platform represents a significant advancement in robotic-assisted surgery, particularly in minimally invasive procedures for lung and kidney applications.</p>
                            <p>As a senior mechanical engineer currently working at J&J, I am mainly responsible for system design on the Monarch Surgical Platform, the first robotic platform for minimally invasive diagnostic and therapeutic procedures in the lungs and kidneys.</p>
                            <p>I work on many projects related to the robotic arm and instrument manipulator, both on the hardware and software front. I'm not able to disclose too many details on these projects at the moment, but I will be glad to share my experience when the legal effect expires.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Auris1.png" alt="Monarch Platform" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Robotic Arm Design',
                    'Medical Device Design',
                    'Robot Arm Calibration',
                    'NPS Software Development',
                    'System Integration'
                ],
                // development: `
                //     <p>Development of the Monarch Platform involved extensive cross-disciplinary collaboration...</p>
                // `,
                // description: `
                //     <p>The Monarch Platform combines advanced robotics and intuitive user interfaces...</p>
                // `
            }
        },
        'yomi': {
            title: 'Yomi Dental Surgical Robot',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                                <p>At Neocis, I was instrumental in developing the second generation of the Yomi surgical system, the only FDA-cleared robotic device for dental implant surgery. My contributions included:</p>
                                <ul>
                                    <li>Design and development of joint actuators for a 7-DOF robotic arm</li>
                                    <li>Creation of compact actuator systems and collaboration with OEMs for customization</li>
                                    <li>Implementation of kinematic analysis and simulation for joint load optimization</li>
                                    <li>Development of calibration algorithms and inverse kinematics solver with obstacle avoidance capabilities</li>
                                </ul>
                                <p>The Yomi system provides haptic guidance to dental surgeons while maintaining their direct control of the surgical instruments, combining the precision of robotic technology with the expertise of human practitioners.</p>  
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Neocis1.png" alt="Yomi Dental Robot" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Robotic Actuator Design',
                    'Precision Mechatronics',
                    'Robot Calibration',
                    'System Integration'
                ],
                // development: `
                //     <p>Development of the Yomi Robot emphasized precision and haptic feedback...</p>
                // `,
                // description: `
                //     <p>The Yomi Robot provides haptic guidance to dental surgeons...</p>
                // `
            }
        },
        'harmony': {
            title: 'Harmony SHR Exoskeleton',
            sections:{
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                                <p>During my time at Harmonic Bionics, I contributed to the development of the Harmony SHR, an advanced upper extremity rehabilitation robot. My responsibilities included:</p>
                                <ul>
                                    <li>Design of major mechanical components using various manufacturing techniques</li>
                                    <li>Development of handheld interface devices and custom linear actuator systems</li>
                                    <li>Material validation through simulation and experimental testing</li>
                                    <li>Integration of mechanical and electrical systems for prototype development</li>
                                </ul>
                                <p>The Harmony SHR is designed to facilitate natural movement patterns during rehabilitation, allowing therapists to deliver high quality care while collecting objective patient data.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/HB1.png" alt="Harmony SHR" loading="lazy">
                        </div>
                    </div>
                    `,
                technology: [
                    'Exoskeleton Design',
                    'Rehabilitation Robotics',
                    'Linear Actuators',
                    'Human-Robot Interface'
                ],
                // keyFeatures: [
                //     'Upper Extremity Rehabilitation',
                //     'Custom Linear Actuators',
                //     'Patient Data Collection',
                //     'Natural Movement Patterns'
                // ]
            }
        },
        'athena': {
            title: 'Athena Biomimetic Robot',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>As team lead for the Athena project at Georgia Tech's LIDAR Lab, I managed the development of a complex biomimetic upper body robot:</p>
                            <ul>
                                <li>Led a team of 12 undergraduate students in system design and integration</li>
                                <li>Implemented a 28-DOF system using linear actuators to mimic human muscle architecture</li>
                                <li>Managed CAD repositories and performed topology optimization for weight reduction</li>
                                <li>Developed control systems for coordinated movement of multiple actuators</li>
                            </ul>
                            <p>The Athena project aims to advance our understanding of human-like movement and control in robotic systems, with potential applications in prosthetics and human-robot interaction.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/LIDAR1.png" alt="Athena Biomimetic Robot" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Biomimetic Design',
                    'Linear Actuation',
                    'CAD Optimization',
                    'System Integration'
                ],
                keyFeatures: [
                    '28-DOF System',
                    'Muscle-like Actuation',
                    'Topology Optimization',
                    'Team Leadership'
                ]
            }
        },
        'hip-exo': {
            title: 'Assistive Hip Exoskeleton',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>Working in the EPIC Lab at Georgia Tech, I contributed to the development of an assistive hip exoskeleton designed to reduce the metabolic cost of walking:</p>
                            <ul>
                                <li>Design and fabrication of series elastic actuator housings and components</li>
                                <li>Development of experimental protocols for system characterization</li>
                                <li>Collection and analysis of metabolic cost data under various conditions</li>
                                <li>Integration of sensors and control systems for real-time assistance</li>
                            </ul>
                            <p>This research aimed to develop more effective assistive devices for individuals with mobility impairments, with a focus on improving energy efficiency during walking.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/EPIC1.png" alt="Assistive Hip Exoskeleton" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Exoskeleton Design',
                    'Series Elastic Actuators',
                    'Biomechanics',
                    'Data Collection'
                ],
                keyFeatures: [
                    'Metabolic Cost Reduction',
                    'Custom Actuator Design',
                    'Performance Analysis',
                    'User Testing'
                ]
            }
        },
        'dexterous-hand': {
            title: 'Biomimetic Dexterous Hand',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>The Biomimetic cable-driven dexterous hand is my winter 2025 robotic project work-in-progress.</p>
                            <p>The goal is to develop a hand that mimics human dexterity, using cable-driven phalangeal joints to minimize distal inertia.</p>
                            <p>As of today, I've completed a few iterations of prototypes. Stay tuned for updates on this exciting project!</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/phalanx.gif" alt="Biomimetic Dexterous Hand" loading="lazy">
                        </div>
                    </div>
                `,
                'Phalanx Design': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/iterations.png" alt="Phalanx iterations" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>Development of the Hand involves heavy iterative design and testing. To design and develop the optimal phalanx, I used a 4-bar linkage system to mimic coupled movement of distal and middle interphalangeal joints.</p>
                            <p>In addition, I used a tendon-driven system to actuate the phalanx, which allows for a more compact design and better control over the joint movement. I have completed 5 iterations fo the phalanx design and am aiming to freeze the design before the 7th iteration.</p>
                            <p>More to come on its way!</p>
                        </div>
                    </div>
                `,
                'Cable-driven Mechanism':`
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>The cable-driven mechanism is the core of the hand design. It allows for precise control over the joint movement and minimizes the inertia at the distal end of the finger.</p>
                            <p>By using a tendon-drive system, I can actuate the phalanx with minimal force and achieve a more natural movement pattern. The cable-driven mechanism is designed to be compact and lightweight, making it suitable for a dexterous hand.</p>
                            <p>I'm using miniatruized motors and PTFE tubing based cable routing to achieve the desired movement. Stay tuned for more updates!</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/hand.gif" alt="Cable driven mechanism" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Mechatronics Design',
                    'Cable Driven Actuators',
                    'Rapid Prototyping',
                    'Sensor Fusion'
                ],
            }
        },
        'robotic-mini-golf': {
            title: 'Apex Putter - Robotic Mini-golf',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>Mini golf offers a fascinating challenge for robotics. While children rely on intuition and practice to master it, programming a robot to achieve the same goal demands precise control over its movements and timing. A child needs only a putter, a ball, and a hole to score a hole-in-one, but for a robot, careful planning and execution are key.</p>
                            <p>For the Franka Panda robot, this task becomes even more intricate. Although the robot doesn’t feel the pressure of competition, a poorly planned Cartesian path can create significant difficulties. In this setup, the robot must putt the ball with just the right amount of force to successfully reach the hole.</p>
                            <p>In this project, I developed the vision module for the robot, designed the mechanical components for the putter end-effector, wrote the demo tasks, and created the ROS2 package for Apex-Putter.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP.png" alt="Robotic mini-golf" loading="lazy">
                        </div>
                    </div>
                `,
                Vision: `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP_vision2.png" alt="Apriltag system" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>The vision module is one of the two major components of this robot. Using a single RealSense D435 camera, this module fulfills two key functions: registering the robot base in the camera frame and tracking the ball and hole within the same frame.</p>
                            <p>To achieve this, I used OpenCV to process the camera image and AprilTags to locate the robot base. I designed an AprilTag mount to provide an initial estimate of the robot base transformation and performed optical calibration using the Kabsch algorithm to improve accuracy.</p>
                        </div>
                    </div>
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>After determining the initial robot base transformation, I used a YOLOv8 model to detect the ball and hole in the camera frame.</p>
                            <p>To enhance the model’s prediction accuracy, I trained it on a custom dataset created with images captured by the same camera. The trained model then identified the ball and hole positions in the camera frame.</p>
                            <p>With the camera frame positions of all necessary elements established, I registered them using tf2, enabling me to retrieve any required transformation with a single command.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP_vision3.png" alt="YOLO" loading="lazy">
                        </div>
                    </div>
                `,
                'Mechanical Integration': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP_mech1.png" alt="Mechanical Integration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>To integrate effectively with the Franka FER arm, I designed a custom end-effector specifically for the robot.</p>
                            <p>This end-effector seamlessly interfaces with off-the-shelf putter components. The design underwent several iterations to improve alignment, simplify kinematic planning, and optimize the putter face for smoother motion.</p>
                        </div>
                    </div>
                `,
                'Motion Control': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>The motion control module leverages ROS2 to interface with the robot and the vision module.</p>
                            <p>Using data from the vision module, the motion control module calculates the robot’s end-effector position and orientation, then plans a precise vector path to move the putter toward the ball and hole.</p>
                            <p>As the path is planned, the node uses MoveIt2! package to send the kinematic planning to the robot, which executes the motion accordingly.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/apex-putter.gif" alt="Motion Control" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Kinematic Control',
                    'Computer Vision',
                    'Machine Learning',
                    'YOLO'
                ],
            }
        },
        'grab-a-pen': {
            title: 'Robot Arm Grab-a-pen - MSR Hackathon',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>As the final challenge of the MSR Hackathon, we were presented a challenge to use a RealSense 435D RGBD camera, and a Trossen PincherX100 robot arm to locate and grab a pen in the arm's workspace.</p>
                            <p>Although the task seemed simple, it required thorough understanding of the robot's kinematics, the camera's calibration, and the integration of the two systems.</p>
                            <p>For this project, I developed a custom calibration system for the robot arm and the camera, and write up the motion control sequence in Python. As a bonus, I worked with my cohort mate <a href="https://absrat.com/">Zhengxiao Han</a> and passed the pen between our robotic arms.</p>                           
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/grab-a-pen/setup.png" alt="MSR Challenge" loading="lazy">
                        </div>
                    </div>
                `,
                'Vision and Calibration': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/grab-a-pen/calibration.gif" alt="Optical calibration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>To acquire the position of the pen in the camera frame, I used the OpenCV library to process the camera image and locate the pen through HSV color thresholding. This allows me to deproject the position of the centroid of the pen in purple in the 3D space</p>
                            <p>In order to register the camera frame to the robot base frame, I used the pen itself as a marker, and performed an optical calibration by moving the pen around in the robot end-effector. Having collected a set of corresponding robot points and camera points, I then registered them together using Kabsch algorithm to get the optimal transformation between the camera and the robot.</p>
                        </div>
                    </div>
                `,
                'Motion Control': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>Trossen Robotics provides a Python API for the PincherX100 robot arm, which allows me to send joint commands to the robot arm directly.</p>
                            <p>Using the camera frame position of the pen, I calculated the inverse kinematics of the robot arm to reach the pen position. I then sent the joint commands to the robot arm to move the end-effector to the pen position.</p>
                            <p>After the robot arm reached the pen, I programmed the robot to grasp the pen and pass it to another robot arm. This required precise control over the robot's end-effector and joint positions.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/grab-a-pen/execute.gif" alt="Motion Control" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Computer Vision',
                    'Kinematic calibration',
                    'Hand-eye registration',
                    'Kinematic control'
                ],
            }
        },
        'mobile-manipulation': {
            title: 'Mobile Manipulation - ME449 Robotic Manipulation',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p> Mobile Manipulation was the final challenge of the ME449: Robotic Manipulation course at Northwestern University.</p>
                            <p> Using robotic manipulation, forward kinematics, and linear control that we learned in the classroom, I created a simulation in CoppeliaSim that showcased a KUKA youBot moving a box from one location to another with precision and efficiency.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/mobile-manipulation/youbot.jpg" alt="Kuka youBot" loading="lazy">
                        </div>
                    </div>
                `,
                'Path Planning': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/mobile-manipulation/gripper.png" alt="Youbot gripper configuration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>Given a desired initial and end target position and orientation, I first generated desired trajectory of the target using quintic polynomial interpolation.</p>
                            <p>Then, I constructed the body jacobian of the KUKA youBot and used its pseudoinverse to calculate the configurations required to reach the target position from the actual initial robot configuration.</p>
                        </div>
                    </div>
                `,
                'Feedforward Control': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>Using the calculated joint configurations, I designed a PI controller to move the robot from its initial configuration to the desired configuration. The controller was tuned to minimize the 6-dimensional motion control errors and ensure smooth motion.</p>
                            <p>The simulation works by taking the difference between the desired and actual joint configurations, and using the PI controller to generate the control signal to move the robot to the desired configuration through Euler integration.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/mobile-manipulation/errors.png" alt="Control errors" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Robotic Manipulation',
                    'Forward Kinematics',
                    'Linear Control',
                    'Simulation'
                ],
            }
        }
    };
    
    // Updated openModal function
    document.addEventListener('DOMContentLoaded', () => {
        const modal = document.getElementById('project-modal');
        const modalTitle = modal.querySelector('.modal-title');
        const modalDescription = modal.querySelector('.modal-description');
        const closeButton = modal.querySelector('.modal-close');
        const overlay = modal.querySelector('.modal-overlay');

        function openModal(projectId) {
            const project = projectDetails[projectId];
            if (project) {
                modalTitle.textContent = project.title;

                // Generate modal content dynamically
                let modalContent = '';
                for (const [sectionKey, sectionValue] of Object.entries(project.sections)) {
                    if (sectionKey === 'overview') {
                        modalContent += sectionValue; // Use the overview's preformatted structure
                    } else if (sectionKey === 'technology') {
                        modalContent += `
                            <div class="modal-section">
                                <h3 class="modal-section-title">Technology Stack</h3>
                                <div class="tech-list">
                                    ${sectionValue.map(tech => `<span class="tech-item">${tech}</span>`).join('')}
                                </div>
                            </div>
                        `;
                    } else {
                        modalContent += `
                            <div class="modal-section">
                                <h3 class="modal-section-title">${capitalize(sectionKey)}</h3>
                                <div>${sectionValue}</div>
                            </div>
                        `;
                    }
                }

                modalDescription.innerHTML = modalContent;
                modal.classList.add('active');
                document.body.classList.add('modal-open');
            }
        }

        function closeModal() {
            modal.classList.remove('active');
            document.body.classList.remove('modal-open');
        }

        function capitalize(str) {
            return str.charAt(0).toUpperCase() + str.slice(1);
        }

        // Add click listeners to project items
        document.querySelectorAll('.project-item').forEach(item => {
            item.addEventListener('click', () => {
                const projectId = item.getAttribute('data-project-id');
                openModal(projectId);
            });
        });

        // Close modal when clicking close button or overlay
        closeButton.addEventListener('click', closeModal);
        overlay.addEventListener('click', closeModal);

        // Close modal on escape key press
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape') {
                closeModal();
            }
        });
    });
</script>
