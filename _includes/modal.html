<!-- Modal window -->
<div id="project-modal" class="modal">
    <div class="modal-overlay"></div>
    <div class="modal-content">
        <button class="modal-close">&times;</button>
        <div class="modal-body">
            <h2 class="modal-title"></h2>
            <div class="modal-external-links"></div>
            <div class="modal-description"></div>
        </div>
    </div>
</div>

<script>
    const projectDetails = {
        'monarch': {
            title: 'Monarch Endoscopic Surgical Platform',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                                <p>The Monarch Platform is a major breakthrough in robotic-assisted surgery, especially for minimally invasive procedures involving the lungs and kidneys.</p>
                                <p>As a senior mechanical engineer currently working at J&J, I am mainly responsible for system development of the Monarch Surgical Platform, the first robotic platform for minimally invasive diagnostic and therapeutic procedures in the lungs and kidneys.</p>
                                <p>I work on a variety of projects focused on the robotic arm and instrument manipulator, addressing both hardware and software challenges. Additionally, I support the hardware team in advancing the overall platform development.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Auris1.png" alt="Monarch Platform" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Robotic Arm Design',
                    'Medical Device',
                    'Robot Arm Calibration',
                    'NPS Software Development'
                ],
                externalLinks: [
                    { text: 'Company Website', url: 'https://www.jnjmedtech.com/en-US/product-family/monarch/' }
                ]
            }
        },
        'yomi': {
            title: 'Yomi Dental Surgical Robot',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                                <p>At Neocis, I was heavily involved in developing the second generation of the Yomi surgical system, the only FDA-cleared robotic device for dental implant surgery. My major contributions included:</p>
                                <ul>
                                    <li>Design and development of joint actuators for a 7-DOF robotic arm</li>
                                    <li>Creation of compact robot end-effector for tool integration and user interaction</li>
                                    <li>Implementation of kinematic analysis and simulation for joint load optimization</li>
                                    <li>Development of calibration algorithms and inverse kinematics solver to enable obstacle avoidance capabilities</li>
                                </ul>
                                <p>The Yomi system provides haptic guidance to dental surgeons while maintaining their direct control of the surgical instruments, combining the precision of robotic technology with the expertise of human practitioners.</p>  
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Neocis1.png" alt="Yomi Dental Robot" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Robotic Actuator Design',
                    'Precision Mechatronics',
                    'Robot Calibration',
                    'System Integration'
                ],
                externalLinks: [
                    { text: 'Company Website', url: 'https://www.neocis.com/' }
                ]
            }
        },
        'harmony': {
            title: 'Harmony SHR Exoskeleton',
            sections:{
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                                <p>During my time at Harmonic Bionics, I contributed to the development of the Harmony SHR, an advanced upper extremity rehabilitation robot. My responsibilities included:</p>
                                <ul>
                                    <li>Design of major mechatronic systems using various manufacturing techniques</li>
                                    <li>Development of handheld interface devices and custom linear actuator systems</li>
                                    <li>Material validation through simulation and experimental testing</li>
                                    <li>Integration of mechanical and electrical systems for prototype development</li>
                                </ul>
                                <p>The Harmony SHR is designed to facilitate natural movement patterns during rehabilitation, allowing therapists to deliver high quality care while collecting objective patient data.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/HB1.png" alt="Harmony SHR" loading="lazy">
                        </div>
                    </div>
                    `,
                technology: [
                    'Exoskeleton Design',
                    'Rehabilitation Robotics',
                    'Linear Actuators',
                    'Human-Robot Interface'
                ],
                externalLinks: [
                    { text: 'Company Website', url: 'https://harmonicbionics.com/' }
                ]
                // keyFeatures: [
                //     'Upper Extremity Rehabilitation',
                //     'Custom Linear Actuators',
                //     'Patient Data Collection',
                //     'Natural Movement Patterns'
                // ]
            }
        },
        'athena': {
            title: 'Athena Biomimetic Robot',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>As team lead for the Athena project at Georgia Tech's LIDAR Lab, I managed the development of a complex biomimetic upper body robot:</p>
                            <ul>
                                <li>Led a team of 7 undergraduate students in system design and integration</li>
                                <li>Implemented a 28-DOF system using linear actuators to mimic human muscle architecture</li>
                                <li>Managed CAD repositories and performed topology optimization for weight reduction</li>
                            </ul>
                            <p>The Athena project aims to advance our understanding of human-like movement and control in robotic systems, with potential applications in prosthetics and human-robot interaction.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/LIDAR1.png" alt="Athena Biomimetic Robot" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Biomimetic Design',
                    'Linear Actuation',
                    'CAD Optimization',
                    'System Integration'
                ],
                externalLinks: [
                    { text: 'Lab Homepage', url: 'https://lab-idar.gatech.edu/' }
                ]
                // keyFeatures: [
                //     '28-DOF System',
                //     'Muscle-like Actuation',
                //     'Topology Optimization',
                //     'Team Leadership'
                // ]
            }
        },
        'hip-exo': {
            title: 'Assistive Hip Exoskeleton',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>Working in the EPIC Lab at Georgia Tech, I contributed to the development of an assistive hip exoskeleton designed to reduce the metabolic cost of walking:</p>
                            <ul>
                                <li>Design and fabrication of series elastic actuator housings and components</li>
                                <li>Development of experimental protocols for system characterization</li>
                                <li>Collection and analysis of metabolic cost data under various conditions</li>
                            </ul>
                            <p>This research aimed to develop more effective assistive devices for individuals with mobility impairments, with a focus on improving energy efficiency during walking.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/EPIC1.png" alt="Assistive Hip Exoskeleton" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Exoskeleton Design',
                    'Series Elastic Actuators',
                    'Biomechanics',
                    'Data Collection'
                ],
                externalLinks: [
                    { text: 'Lab Homepage', url: 'https://www.epic.gatech.edu/' }
                ]
                // keyFeatures: [
                //     'Metabolic Cost Reduction',
                //     'Custom Actuator Design',
                //     'Performance Analysis',
                //     'User Testing'
                // ]
            }
        },
        'dexterous-hand': {
            title: 'BiDexHand: 16-DoF Open-Source Biomimetic Dexterous Hand',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>The human hand, with a complex tendon-muscle network, and over 20 degrees of freedom, is exceptionally dexterous. Inspired by this marvel, I'm designing a robotic hand to replicate its form and function.</p>
                            <p>Key features:</p>
                            <ul>
                                <li>Anthropomorphic phalanx design with <strong>16 total DoF</strong> and 21 Joints</li> 
                                <li><strong>Cable-driven</strong> mechanism for precise control and minimal inertia</li> 
                                <li>Motion shadowing to mimic hand gestures with multi-modal input</li> 
                                <li><strong>ROS2</strong> based control framework for seamless integration</li>
                            </ul>
                            <p>Started in January 2025, the Biomimetic Dexterous Hand project is a work-in-progress. I will continue to post updates to the project here as it develops.</p>
                        </div>
                        <div class="overview-image">
                            <iframe width="400" height="400" src="https://www.youtube.com/embed/X8zVKlZNorc?si=oCd5qKsHgr6xLeVZ&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                        </div>
                    </div>
                `,
                'Phalanx Design': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/phalanx_evolution.gif" alt="Phalanx design" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>I designed an anti-parallelogram 4-bar linkage system to replicate the coupled movement of the distal and middle interphalangeal joints. The design decision was made to mimic the natural rhythm of the human finger while reducing the actuator count by one per phalanx, streamlining the design without sacrificing functionality.</p>
                            <p>After five iterations, I arrived at the current phalanx design that balances robust strength with an extensive range of motion. Each prototype iteration provided critical feedback, allowing for adjustments that enhance the hand's reliability and dynamic performance in interacting with its environment.</p>
                        </div>
                    </div>
                `,
                'Cable-driven Mechanism': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>The cable-driven mechanism is the heart of the hand design. It provides precise control over joint movement while minimizing distal inertia - a crucial factor for achieving anthropomorphic dexterity.</p>
                            <p>I used an <strong>'N-configuration'</strong> tendon routing system that allows for a compact layout. While this tendon configuration is not antagonistic, I designed a tensioning system to ensure that the wires are always under tension, reducing backlash and improving joint accuracy.</p>
                            <p>To achieve the desired movement, I'm using miniaturized motors and PTFE tubing-based cable routing. This combination balances between friction and path complexity, and maintains a design that is both modular and efficient for executing complex tasks.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/tendon_drive.gif" alt="Biomimetic Dexterous Hand" loading="lazy">
                        </div>
                    </div>
                `,
                'RGB Camera-based Motion Shadowing': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/motion_shadowing.gif" alt="Motion Shadowing" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>One of the key features of the Hand is its ability to shadow human hand movements. By using computer vision and machine learning algorithms, the Hand can mimic human gestures with high precision.</p>
                            <p>To implement this feature, I wrote a ROS2 node that uses Google's <a href="https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker" target="_blank" rel="noopener noreferrer" class="custom-link">MediaPipe</a> framework to acquire hand landmark position and calculate joint angles via vector math. These computed angles are then published to the Hand's servo controller through a hand angle topic, ensuring seamless synchronization between human intent and robotic motion.</p>
                        </div>
                    </div>
                `,
                'Alternative Input Modality: VR Control':`
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>While the Hand can shadow human movements, it also supports different input modalities. With Unity Editor, I created <a href="https://github.com/NU-MECH-ENG-495/project-wengmister-vr-hand-tracking" target="_blank" rel="noopener noreferrer" class="custom-link">an VR app</a> for a Meta Quest 3S headset that calculates and broadcasts tracked hand joint angles, enabling very intuitive control over the robotic hand with HUD debugging info and synthetic overlay.</p> 
                            <p>With this feature, the Hand can be used in a variety of applications, from teleoperation to virtual reality experiences. The Hand's flexibility in input modalities ensures seamless integration into diverse environments and use cases.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/vr_control.gif" alt="VR Control" loading="lazy">          
                        </div>
                    </div>  
                `,
                'Improving Accuracy and Controllability':`
                    <div class="modal-section overview-section">    
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/v2_improvements.gif" alt="V2 Improvements" loading="lazy">
                        </div>
                    <div class="overview-text">
                        <p>In the later half of the project, I revemped the mechanical architecture to improve joint range of motion and accuracy. <p>
                        <p>By redesigning cable routing inside the phalanx linkages, I was able to expand the MCP joint range of motion by more than 100%. This improvement significantly enhances the Hand's dexterity and control bandwidth, making it more versatile for a wide range of applications.</p>
                        <p>Also, I realized during builds that <strong>greater tendon path variability leads to greater joint angle hysteresis</strong>. Therefore, I implemented a novel cable tensioning system that maintains consistent tension across the tendon path in addition to constraining the PTFE tubes, greatly reducing hysteresis and improving joint accuracy.</p>  
                    </div>
                </div>
                `,
                'URDF: Simulation and Visualization':`
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>For simulation and visualization, I created a URDF model of the Hand in ROS2. This model accurately represents the Hand's mechanical structure and kinematics, enabling realistic simulation and visualization of the Hand's movements.</p>
                            <p>By integrating the URDF model with Rviz, I can simulate and visualize the Hand's motion and test its performance in real time. This capability allows me to refine the Hand's design and control scheme before implementing them on the physical prototype.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/urdf_sim.gif" alt="URDF Simulation" loading="lazy">
                        </div>
                    </div>
                `,
                'Arm Integration: Franka FER Robot':`
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/franka_integration_2.gif" alt="Franka Integration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>As the very natural next step, I integrated my robotic hand with a robotic arm - the <a href="https://franka.de/" target="_blank" rel="noopener noreferrer" class="custom-link">Franka Emika Research robot</a>. This integration allows the Hand to be used for more complex tasks, such as dexterous manipulation and object grasping.</p>
                            <p>In addition, I designed a Intel RealSense D405 wrist camera in the hand-arm coupling, which enables the Hand to perform vision-based tasks and interact with the environment more effectively.</p>
                            <p>With the popularity of the FER arm, and the open-source nature of this project, I created a relatively low-cost yet easily accessible dexterity platform for research and development in robotics and automation.</p>
                        </div>
                    </div>  
                `,
                'Benchmarking: Force and Dexterity Evaluation':`
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>To assess BiDexHand's real-world performance, I ran a series of benchmarks targeting both dexterity and force capabilities.</p>
                            <p>For dexterity, I used the GRASP Taxonomy to evaluate the hand's ability to perform 33 common grasp types and the Kapandji score to measure thumb reach across 11 positions. BiDexHand successfully achieved all 33 grasps and 9 out of 11 thumb poses, highlighting its flexibility and range of motion.</p>
                            <p>Though not specifically built for high force output, the hand demonstrated solid performance in force tests - it was able to lift a 10lb dumbbell using a hook grasp and deliver an average fingertip force of 2.14N per phalanx.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/benchmarking.gif" alt="Dexterity Evaluation" loading="lazy">
                        </div>
                    </div>
                `,
                'Control Architecture':`
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/control_architecture.png" alt="Control Architecture" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>The control architecture for the Hand is designed and implemented with ROS2, allowing for easy integration with the pub-sub systems.</p>
                            <p>A few ROS2 nodes work together to implement the control: vision node integrates MediaPipe (or other motion input source) and processes camera input; application node processes joint angles and compensates them according to robot calibration; and finally the servo node sends the corresponding commands to the Hand's servo controller via serial comms.</p>
                        </div>
                    </div>
                `,
                'Future Work': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>The next phase of the project will focus on enhancing the Hand's reliability and performance, making it more intelligent and more robust. I'm planning on implementing:</p>
                            <ul>
                                <li><strong>Vision-based</strong> closed-loop position control</li>
                                <ul>
                                    <li>Servo calibration</li>
                                    <li>Key-point based joint pose estimation</li>
                                </ul>
                                <li>Touch sensing</li>
                                <li>Learning-based optimal grasping and task planning</li>
                                <li>Full arm visual servoing teleoperation with Franka FER</li>
                            </ul>
                            <p>Stay tuned for more updates and demos on this exciting journey! If you have other interesting ideas or would like to contribute to this project, please don't hesitate to <a href="mailto:wengmister@gmail.com" class="custom-link">reach out to me</a>.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/future_dev.png" alt="future work" loading="lazy">
                        </div>
                    </div>
                `,
                'Acknowledgement': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Hand/thumbup.gif" alt="design_iterations" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>This project was delivered through a heavily iterative design process. With each new prototype I've gained valuable insights and feedback that have helped me refine the Hand's design and functionality.</p> 
                            <p>Special thanks to Northwestern University's <a href="https://design.northwestern.edu/info-by-audience/current-students/rapid-prototyping-lab.html" target="_blank" class="custom-link">RP Lab </a> for their support in 3D printing for this project.</p>
                        </div>
                    </div>
                `,
                technology: [
                    'Mechatronics Design',
                    'Cable Driven Actuators',
                    'Rapid Prototyping',
                    'Computer Vision',
                    'Virtual Reality',
                    'ROS2'
                ],
                externalLinks: [
                    { text: 'Github Repo', url: 'https://github.com/wengmister/Dex_Hand_MSR' },
                    { text: '      arXiv', url: 'https://arxiv.org/abs/2504.14712'}
                ]
            }
        },
        'robotic-mini-golf': {
            title: 'Apex Putter - Robotic Mini-golf',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>Mini golf offers a fascinating challenge for robotics. While children rely on intuition and practice to master it, programming a robot to achieve the same goal demands precise control over its movements and timing. A child needs only a putter, a ball, and a hole to score a hole-in-one, but for a robot, careful planning and execution are key.</p>
                            <p>For the Franka Panda robot, this task becomes even more intricate. Although the robot doesn’t feel the pressure of competition, a poorly planned Cartesian path can create significant difficulties. In this setup, the robot must putt the ball with just the right amount of force to successfully reach the hole.</p>
                            <p>In this project, I developed the vision module for the robot, designed the mechanical components for the putter end-effector, wrote the demo tasks, and created the ROS2 package for Apex-Putter.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP.png" alt="Robotic mini-golf" loading="lazy">
                        </div>
                    </div>
                `,
                Vision: `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP_vision2.png" alt="Apriltag system" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>The vision module is one of the two major components of this robot. Using a single RealSense D435 camera, this module fulfills two key functions: registering the robot base in the camera frame and tracking the ball and hole within the same frame.</p>
                            <p>To achieve this, I used OpenCV to process the camera image and AprilTags to locate the robot base. I designed an AprilTag mount to provide an initial estimate of the robot base transformation and performed optical calibration using the Kabsch algorithm to improve accuracy.</p>
                        </div>
                    </div>
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>After determining the initial robot base transformation, I used a YOLOv8 model to detect the ball and hole in the camera frame.</p>
                            <p>To enhance the model’s prediction accuracy, I trained it on a custom dataset created with images captured by the same camera. The trained model then identified the ball and hole positions in the camera frame.</p>
                            <p>With the camera frame positions of all necessary elements established, I registered them using tf2, enabling me to retrieve any required transformation with a single command.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP_vision3.png" alt="YOLO" loading="lazy">
                        </div>
                    </div>
                `,
                'Mechanical Integration': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/AP_mech1.png" alt="Mechanical Integration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>To integrate effectively with the Franka FER arm, I designed a custom end-effector specifically for the robot.</p>
                            <p>This end-effector seamlessly interfaces with off-the-shelf putter components. The design underwent several iterations to improve alignment, simplify kinematic planning, and optimize the putter face for smoother motion.</p>
                        </div>
                    </div>
                `,
                'Motion Control': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>The motion control module leverages ROS2 to interface with the robot and the vision module.</p>
                            <p>Using data from the vision module, the motion control module calculates the robot’s end-effector position and orientation, then plans a precise vector path to move the putter toward the ball and hole.</p>
                            <p>As the path is planned, the node uses MoveIt2! package to send the kinematic planning to the robot, which executes the motion accordingly.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/Apex-Putter/apex-putter.gif" alt="Motion Control" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Kinematic Control',
                    'Computer Vision',
                    'Machine Learning',
                    'YOLO'
                ],
                externalLinks: [
                    { text: 'Project Repo', url: 'https://github.com/wengmister/Apex-Putter'}
                ]
            }
        },
        'grab-a-pen': {
            title: 'Robot Arm Grab-a-pen - MSR Hackathon',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>For the final challenge of the MSR Hackathon, we were tasked with using a RealSense 435D RGBD camera and a Trossen PincherX100 robot arm to locate and grab a pen within the arm's workspace.</p>
                            <p>Though the task seemed straightforward, it required a thorough understanding of the robot's kinematics, the camera's calibration, and the integration of both systems.</p>
                            <p>For this project, I developed a custom calibration system for both the robot arm and the camera, and wrote the motion control sequence in Python. As a bonus, I collaborated with my cohort mate <a href="https://absrat.com/" target="_blank">Zhengxiao Han</a> to pass the pen between our robotic arms.</p>                           
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/grab-a-pen/setup.png" alt="MSR Challenge" loading="lazy">
                        </div>
                    </div>
                `,
                'Vision and Calibration': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/grab-a-pen/calibration.gif" alt="Optical calibration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>To determine the position of the pen in the camera's frame, I used the OpenCV library to process the camera image and locate the pen using HSV color thresholding. This enabled me to deproject the centroid of the pen (highlighted in purple) into 3D space.</p>
                            <p>To register the camera frame with the robot base frame, I used the pen itself as a marker. By moving the pen within the robot's end-effector, I performed an optical calibration. After collecting corresponding points from both the robot and the camera, I applied the Kabsch algorithm to calculate the optimal transformation between the two frames.</p>
                        </div>
                    </div>
                `,
                'Motion Control': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>Trossen Robotics provides a Python API for the PincherX100 robot arm, which I used to send joint commands directly to the robot.</p>
                            <p>Using the pen's position in the camera frame, I calculated the inverse kinematics needed for the robot arm to reach the pen. I then sent joint commands to the robot to position the end-effector at the pen's location.</p>
                            <p>Once the robot arm reached the pen, I programmed it to grasp the pen and pass it to another robot arm. This required precise control over both the end-effector and joint positions.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/grab-a-pen/execute.gif" alt="Motion Control" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Computer Vision',
                    'Kinematic calibration',
                    'Hand-eye registration',
                    'Kinematic control'
                ],
                externalLinks: [
                    { text: 'Project Repo', url: 'https://github.com/wengmister/msr_PincherX'}
                ]
            }
        },
        'mobile-manipulation': {
            title: 'Mobile Manipulation - ME449 Robotic Manipulation',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>Mobile Manipulation was the final challenge of the ME449: Robotic Manipulation course at Northwestern University.</p>
                            <p>Using robotic manipulation, forward kinematics, and linear control techniques we learned in class, I created a simulation in CoppeliaSim. The simulation demonstrated a KUKA youBot moving a box from one location to another with precision and efficiency.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/mobile-manipulation/youbot.jpg" alt="Kuka youBot" loading="lazy">
                        </div>
                    </div>
                `,
                'Path Planning': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/mobile-manipulation/gripper.png" alt="Youbot gripper configuration" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>Given the desired initial and final target positions and orientations, I first generated the target trajectory using quintic polynomial interpolation.</p>
                            <p>Next, I constructed the body Jacobian for the KUKA youBot and used its pseudoinverse to calculate the necessary configurations to move the robot from its current state to the target position.</p>
                        </div>
                    </div>
                `,
                'Feedforward Control': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>Using the calculated joint configurations, I designed a PI controller to move the robot from its initial to its desired configuration. The controller was tuned to minimize 6-dimensional motion control errors and ensure smooth motion.</p>
                            <p>The simulation works by computing the difference between the desired and actual joint configurations. The PI controller then generates a control signal to move the robot to the desired configuration through Euler integration.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/mobile-manipulation/errors.png" alt="Control errors" loading="lazy">
                        </div>
                    </div>
                `,
                technology: [
                    'Robotic Manipulation',
                    'Forward Kinematics',
                    'Linear Control',
                    'Simulation'
                ],
                externalLinks: [
                    { text: 'Project Repo', url: 'https://github.com/wengmister/me449/tree/main/final_project'}
                ]
            }
        },
        'point-cloud-classification': {
            title: 'RGB Point Cloud Classification - Machine Learning',
            sections: {
                overview: `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <h3 class="modal-section-title">Overview</h3>
                            <p>RGB Point Cloud Classification was my final project for MSAI 349: Machine Learning.</p>
                            <p>Point clouds are a common data format used in robotics and computer vision. In this project, we implemented a custom PointNet architecture and augmented it with RGB color on the point cloud to classify objects in a custom dataset we generated.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/point-cloud/pcc-overview.gif" alt="Point Cloud Classification" loading="lazy">
                        </div>
                    </div>
                `,
                'PointNet Architecture': `
                    <div class="modal-section overview-section">
                        <div class="overview-image">
                            <img src="assets/images/project_images/point-cloud/pcc-intro.gif" alt="PointNet Introduction" loading="lazy">
                        </div>
                        <div class="overview-text">
                            <p>PointNet is a deep learning architecture designed to process point clouds directly. It is invariant to input permutation and can learn local features from unordered point sets.</p>
                            <p>For our project, we implemented a custom PointNet architecture using PyTorch. The network was trained on a custom dataset of point clouds augmented with RGB color information.</p>
                        </div>
                    </div>
                `,
                'RGB Augmentation': `
                    <div class="modal-section overview-section">
                        <div class="overview-text">
                            <p>RGB color information provides valuable context for object classification in point clouds. By augmenting our dataset with RGB values, we enhanced the network's ability to distinguish between objects of similar shape.</p>
                            <p>During training, we concatenated the RGB values with the point cloud data and fed the combined input to the PointNet architecture. This allowed the network to learn color-based features in addition to geometric features.</p>
                        </div>
                        <div class="overview-image">
                            <img src="assets/images/project_images/point-cloud/pcc-architecture.png" alt="RGB Augmentation" loading="lazy">
                        </div>
                    </div>
                `,
                'Acknowledgement': `
                    <div class="modal-section overview-section">
                            <p>Special thanks to my project partners, <a href="https://absrat.com/" target="_blank">Zhengxiao Han</a>, <a href="https://benbenyamin.github.io/" target="_blank">Ben Benyamin</a> and mentor <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/demeter-david.html" target="_blank">Prof. David Demeter</a> for their collaboration and contributions to this project.</p>
                    </div>
                `,
                technology: [
                    'Machine Learning',
                    'Multilayer Perceptron',
                    'PointNet',
                    'RGB Augmentation'
                ],
                externalLinks: [
                    { text: 'Project Repo', url: 'https://github.com/msr-in-msai/msai-349-final-project'}
                ]
            }
        }
    };
    
    document.addEventListener('DOMContentLoaded', () => {
        const modal = document.getElementById('project-modal');
        const modalTitle = modal.querySelector('.modal-title');
        const modalExternalLinks = modal.querySelector('.modal-external-links');
        const modalDescription = modal.querySelector('.modal-description');
        const closeButton = modal.querySelector('.modal-close');
        const overlay = modal.querySelector('.modal-overlay');

        function openModal(projectId) {
        const project = projectDetails[projectId];
        if (project) {
            // Set modal title
            modalTitle.textContent = project.title;

            // Render external links (if available) right below the title
            if (project.sections.externalLinks) {
            modalExternalLinks.innerHTML = project.sections.externalLinks
                .map(link => `<a href="${link.url}" target="_blank" class="contact-item" style="margin-right: 20px;">${link.text}</a>`)
                .join(' ');
            } else {
            modalExternalLinks.innerHTML = '';
            }

            // Generate the remaining modal content dynamically
            let modalContent = '';
            for (const [sectionKey, sectionValue] of Object.entries(project.sections)) {
            // Skip externalLinks since it's handled separately
            if (sectionKey === 'externalLinks') continue;

            if (sectionKey === 'overview') {
                modalContent += sectionValue; // Use preformatted overview
            } else if (sectionKey === 'technology') {
                modalContent += `
                <div class="modal-section">
                    <h3 class="modal-section-title">Technology Stack</h3>
                    <div class="tech-list">
                    ${sectionValue.map(tech => `<span class="tech-item">${tech}</span>`).join('')}
                    </div>
                </div>
                `;
            } else {
                modalContent += `
                <div class="modal-section">
                    <h3 class="modal-section-title">${capitalize(sectionKey)}</h3>
                    <div>${sectionValue}</div>
                </div>
                `;
            }
            }
            modalDescription.innerHTML = modalContent;

            modal.classList.add('active');
            document.body.classList.add('modal-open');

            // Update the URL hash
            history.pushState({ projectId: projectId }, '', `#${projectId}`);
        }
        }

        function closeModal() {
        modal.classList.remove('active');
        document.body.classList.remove('modal-open');
        // Revert the URL
        history.pushState(null, '', window.location.pathname);
        }

        function capitalize(str) {
        return str.charAt(0).toUpperCase() + str.slice(1);
        }

        // Add click listeners to project items
        document.querySelectorAll('.project-item').forEach(item => {
        item.addEventListener('click', () => {
            const projectId = item.getAttribute('data-project-id');
            openModal(projectId);
        });
        });

        // Close modal when clicking the close button or overlay
        closeButton.addEventListener('click', closeModal);
        overlay.addEventListener('click', closeModal);

        // Close modal on escape key press
        document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape') {
            closeModal();
        }
        });

        // Handle back button
        window.addEventListener('popstate', (event) => {
        if (event.state && event.state.projectId) {
            openModal(event.state.projectId);
        } else {
            closeModal();
        }
        });

        // Check URL hash on page load
        const initialProjectId = window.location.hash.substring(1);
        if (initialProjectId && projectDetails[initialProjectId]) {
        openModal(initialProjectId);
        }
    });
</script>
